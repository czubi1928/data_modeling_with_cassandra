{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: cassandra-driver in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (3.29.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (from cassandra-driver) (0.2.1.post1)\n",
      "Requirement already satisfied: click in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (8.2.1)\n",
      "Requirement already satisfied: six in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pczub\\pycharmprojects\\data_engineering_projects\\data_modeling_with_cassandra\\.venv\\lib\\site-packages (from click->geomet<0.3,>=0.1->cassandra-driver) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas cassandra-driver numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 files in the event data folder\n"
     ]
    }
   ],
   "source": [
    "# Relative path (assuming your initial project structure)\n",
    "data_folder_path = os.path.join(\n",
    "    os.path.dirname(os.getcwd()), \"data\", \"raw\", \"event_data\"\n",
    ")\n",
    "# print(f'The constructed path is: {data_folder_path}')\n",
    "\n",
    "if not os.path.exists(data_folder_path):\n",
    "    raise FileNotFoundError(\n",
    "        f'The directory \"{data_folder_path}\" was not found. Check the path'\n",
    "    )\n",
    "\n",
    "# Your data processing code here (example)\n",
    "file_paths = []\n",
    "for root, _, files in os.walk(data_folder_path):\n",
    "    for file in files:\n",
    "        file_paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(file_paths)} files in the event data folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 8056\n",
      "Sample data:\n",
      " [['', 'Logged In', 'Walter', 'M', '0', 'Frye', '', 'free', 'San Francisco-Oakland-Hayward, CA', 'GET', 'Home', '1.54092E+12', '38', '', '200', '1.54111E+12', '39'], ['', 'Logged In', 'Kaylee', 'F', '0', 'Summers', '', 'free', 'Phoenix-Mesa-Scottsdale, AZ', 'GET', 'Home', '1.54034E+12', '139', '', '200', '1.54111E+12', '8'], [\"Des'ree\", 'Logged In', 'Kaylee', 'F', '1', 'Summers', '246.30812', 'free', 'Phoenix-Mesa-Scottsdale, AZ', 'PUT', 'NextSong', '1.54034E+12', '139', 'You Gotta Be', '200', '1.54111E+12', '8'], ['', 'Logged In', 'Kaylee', 'F', '2', 'Summers', '', 'free', 'Phoenix-Mesa-Scottsdale, AZ', 'GET', 'Upgrade', '1.54034E+12', '139', '', '200', '1.54111E+12', '8'], ['Mr Oizo', 'Logged In', 'Kaylee', 'F', '3', 'Summers', '144.03873', 'free', 'Phoenix-Mesa-Scottsdale, AZ', 'PUT', 'NextSong', '1.54034E+12', '139', 'Flat 55', '200', '1.54111E+12', '8']]\n"
     ]
    }
   ],
   "source": [
    "# Initiating an empty list of rows that will be generated from each file\n",
    "data_rows = []\n",
    "\n",
    "# For every file in the file path list\n",
    "for file_path in file_paths:\n",
    "    # Reading .csv file\n",
    "    with open(file_path, \"r\", encoding=\"utf8\", newline=\"\") as csv_file:\n",
    "        # Creating a CSV reader object\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        next(csv_reader)  # Skip header\n",
    "\n",
    "        # Extracting each data row one by one and append it\n",
    "        for line in csv_reader:\n",
    "            data_rows.append(line)\n",
    "\n",
    "print(f\"Total rows: {len(data_rows)}\")\n",
    "print(f\"Sample data:\\n {data_rows[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a smaller \"events.csv\" file that will be used to insert data into the Apache Cassandra tables\n",
    "csv.register_dialect(\"myDialect\", quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "event_file = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"events.csv\")\n",
    "\n",
    "with open(event_file, \"w\", encoding=\"utf8\", newline=\"\") as new_file:\n",
    "    writer = csv.writer(new_file, dialect=\"myDialect\")\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"artist\",\n",
    "            \"firstName\",\n",
    "            \"gender\",\n",
    "            \"itemInSession\",\n",
    "            \"lastName\",\n",
    "            \"length\",\n",
    "            \"level\",\n",
    "            \"location\",\n",
    "            \"sessionId\",\n",
    "            \"song\",\n",
    "            \"userId\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for row in data_rows:\n",
    "        if row[0] == \"\":\n",
    "            continue\n",
    "\n",
    "        writer.writerow(\n",
    "            (\n",
    "                row[0],\n",
    "                row[2],\n",
    "                row[3],\n",
    "                row[4],\n",
    "                row[5],\n",
    "                row[6],\n",
    "                row[7],\n",
    "                row[8],\n",
    "                row[12],\n",
    "                row[13],\n",
    "                row[16],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of rows in new \"events.csv\" file\n",
    "with open(event_file, \"r\", encoding=\"utf8\") as file:\n",
    "    print(sum(1 for line in file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Working with <font color=red>events.csv</font> file, which contains the following columns: \n",
    "- artist \n",
    "- firstName\n",
    "- gender\n",
    "- itemInSession\n",
    "- lastName\n",
    "- length\n",
    "- level\n",
    "- location\n",
    "- sessionId\n",
    "- song\n",
    "- userId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Cassandra!\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "try:\n",
    "    # Connect to the local Cassandra cluster\n",
    "    cluster = Cluster([\"127.0.0.1\"])  # Replace with your Cassandra node IPs if needed\n",
    "    session = cluster.connect()\n",
    "\n",
    "    print(\"Successfully connected to Cassandra!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to Cassandra: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating Keyspace\n",
    "keyspace_query = \"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS sparkify \n",
    "    with REPLICATION = \n",
    "    {'class': 'SimpleStrategy', 'replication_factor': 1}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.execute(keyspace_query)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create keyspace: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Setting Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Setting KEYSPACE to the keyspace specified above\n",
    "session.set_keyspace(\"sparkify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Query 1\n",
    "For query 1, we need a way to run query on sessionId and itemInSession. So, our primary key must have these columns. We can partition the data on sessionId.\n",
    "Our Select query: SELECT artist, song, length FROM session_item WHERE sessionId = 338 AND itemInSession = 4;\n",
    "Our Primary key will be (sessionId, itemInSession), where sessionId is the partition key and  itemInSession is the clustering column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating table for query_1\n",
    "query_1 = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS session_item(\n",
    "        artist text,\n",
    "        song text,\n",
    "        length float,\n",
    "        sessionId int,\n",
    "        itemInSession int,\n",
    "        PRIMARY KEY (sessionId, itemInSession)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.execute(query_1)\n",
    "except Exception as e:\n",
    "    print(f\"Table creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reading csv file and inserting rows into cassandra tables\n",
    "with open(event_file, encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    next(csv_reader)  # Skip header\n",
    "\n",
    "    for line in csv_reader:\n",
    "        query = \"\"\"\n",
    "            INSERT INTO session_item\n",
    "                (artist, song, length, sessionId, itemInSession) \n",
    "            VALUES\n",
    "                (%s, %s, %s, %s, %s);\n",
    "        \"\"\"\n",
    "        session.execute(\n",
    "            query, (line[0], line[10], float(line[5]), int(line[8]), int(line[3]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Faithless', song='50', length=495.30731201171875)\n"
     ]
    }
   ],
   "source": [
    "# SELECT statement to verify the data was entered into the table\n",
    "select_query_1 = \"\"\"\n",
    "    SELECT artist, song, length \n",
    "    FROM  session_item \n",
    "    WHERE sessionId = 338 \n",
    "      AND itemInSession = 4\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(select_query_1)\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Query 2\n",
    "For query 2, we need a way to run query on sessionId and userId. Also, we need the data sorted on itemInSession. So, our primary key must have these columns. We can partition the data on a composite key (sessionId, userId).\n",
    "Our Select query : SELECT artist, song, firstName, lastName FROM  user_session WHERE sessionId = 182 AND userId = 10;\n",
    "Our Primary key will be ((sessionId, userId), itemInSession)), where (sessionId, userId) is the partition key and  itemInSession is the clustering column.\n",
    "Also, we are using the clause - WITH CLUSTERING ORDER BY (itemInSession ASC), to sort our data based on itemInSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating table for query2\n",
    "query_2 = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS user_session(\n",
    "        sessionId int,\n",
    "        userId int,\n",
    "        artist text,\n",
    "        song text,\n",
    "        firstName text,\n",
    "        lastName text,\n",
    "        itemInSession int,\n",
    "        PRIMARY KEY ((sessionId, userId), itemInSession)) WITH CLUSTERING ORDER BY (itemInSession ASC\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.execute(query_2)\n",
    "except Exception as e:\n",
    "    print(f\"Table creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(event_file, encoding=\"utf8\") as new_file:\n",
    "    csv_reader = csv.reader(new_file)\n",
    "    next(csv_reader)  # Skip header\n",
    "\n",
    "    for line in csv_reader:\n",
    "        query = \"\"\"\n",
    "            INSERT INTO user_session\n",
    "                (sessionId, userId, artist, song, firstName, lastName, itemInSession)\n",
    "            VALUES\n",
    "                (%s, %s, %s, %s, %s, %s, %s);\n",
    "        \"\"\"\n",
    "        session.execute(\n",
    "            query,\n",
    "            (\n",
    "                int(line[8]),\n",
    "                int(line[10]),\n",
    "                line[0],\n",
    "                line[9],\n",
    "                line[1],\n",
    "                line[4],\n",
    "                int(line[3]),\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Down To The Bone', song=\"Keep On Keepin' On\", firstname='Sylvie', lastname='Cruz')\n",
      "Row(artist='Three Drives', song='Greece 2000', firstname='Sylvie', lastname='Cruz')\n",
      "Row(artist='Sebastien Tellier', song='Kilometer', firstname='Sylvie', lastname='Cruz')\n",
      "Row(artist='Lonnie Gordon', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)', firstname='Sylvie', lastname='Cruz')\n"
     ]
    }
   ],
   "source": [
    "# SELECT statement to verify the data was entered into the table\n",
    "select_query_2 = \"\"\"\n",
    "    SELECT artist, song, firstName, lastName \n",
    "    FROM user_session\n",
    "    WHERE sessionId = 182\n",
    "      AND userId = 10;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(select_query_2)\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Query 3\n",
    "For query 3, we need a way to run query on song. So, our primary key must have song. Also, the query should be such that it does not contain duplicate users for a song. So we need to model data in such a way that we don't allow duplicate users for a song in our table. This can be acheived by including userId in our primary key.\n",
    "Our Select query: SELECT song, firstName, lastName FROM user_song WHERE song = 'All Hands Against His Own';\n",
    "Our Primary key will be ((song), userId)), where song is the partition key and  userId is the clustering column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating table for query_3\n",
    "query_3 = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS user_song(\n",
    "        song text,\n",
    "        userId int,\n",
    "        firstName text,\n",
    "        lastName text,\n",
    "        PRIMARY KEY ((song), userId)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.execute(query_3)\n",
    "except Exception as e:\n",
    "    print(f\"Table creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(event_file, encoding=\"utf8\") as new_file:\n",
    "    csv_reader = csv.reader(new_file)\n",
    "    next(csv_reader)  # Skip header\n",
    "\n",
    "    for line in csv_reader:\n",
    "        query = \"\"\"\n",
    "            INSERT INTO user_song\n",
    "                (song, userId, firstName, lastName)\n",
    "            VALUES\n",
    "                (%s, %s, %s, %s);\n",
    "        \"\"\"\n",
    "        session.execute(query, (line[9], int(line[10]), line[1], line[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Down To The Bone', song=\"Keep On Keepin' On\", firstname='Sylvie', lastname='Cruz')\n",
      "Row(artist='Three Drives', song='Greece 2000', firstname='Sylvie', lastname='Cruz')\n",
      "Row(artist='Sebastien Tellier', song='Kilometer', firstname='Sylvie', lastname='Cruz')\n",
      "Row(artist='Lonnie Gordon', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)', firstname='Sylvie', lastname='Cruz')\n"
     ]
    }
   ],
   "source": [
    "# SELECT statement to verify the data was entered into the table\n",
    "select_query_3 = \"\"\"\n",
    "    SELECT song, firstName, lastName\n",
    "    FROM user_song\n",
    "    WHERE song = 'All Hands Against His Own';\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(select_query_3)\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Drop the tables before closing out the sessions\n",
    "If you want to use, change cell from \"markdown\" to \"code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "session.execute(\"DROP TABLE IF EXISTS session_item\")\n",
    "session.execute(\"DROP TABLE IF EXISTS user_session\")\n",
    "session.execute(\"DROP TABLE IF EXISTS user_song\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Close the session and cluster connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Modeling With Cassandra 3.11",
   "language": "python",
   "name": "data_modeling_with_cassandra_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
